{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dc3bc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"books_with_emotions.json\"\n",
    "output_file = \"books_with_emotions_fixed.json\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"[\\n\")\n",
    "    for i, line in enumerate(lines):\n",
    "        if i < len(lines) - 1:\n",
    "            f.write(line + \",\\n\")\n",
    "        else:\n",
    "            f.write(line + \"\\n\")\n",
    "    f.write(\"]\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ec6154a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Model input names: ['input_layer']\n",
      "ðŸš€ Starting emotion vector update...\n",
      "âœ… Updated 99 books so far...\n",
      "âœ… Updated 194 books so far...\n",
      "âœ… Updated 294 books so far...\n",
      "âœ… Updated 391 books so far...\n",
      "âœ… Updated 490 books so far...\n",
      "âœ… Updated 589 books so far...\n",
      "âœ… Updated 688 books so far...\n",
      "âœ… Updated 786 books so far...\n",
      "âœ… Updated 884 books so far...\n",
      "âœ… Updated 983 books so far...\n",
      "âœ… Updated 1080 books so far...\n",
      "âœ… Updated 1180 books so far...\n",
      "âœ… Updated 1278 books so far...\n",
      "âœ… Updated 1377 books so far...\n",
      "âœ… Updated 1477 books so far...\n",
      "âœ… Updated 1574 books so far...\n",
      "âœ… Updated 1673 books so far...\n",
      "âœ… Updated 1772 books so far...\n",
      "âœ… Updated 1870 books so far...\n",
      "âœ… Updated 1967 books so far...\n",
      "âœ… Updated 2067 books so far...\n",
      "âœ… Updated 2167 books so far...\n",
      "âœ… Updated 2267 books so far...\n",
      "âœ… Updated 2366 books so far...\n",
      "âœ… Updated 2465 books so far...\n",
      "âœ… Updated 2563 books so far...\n",
      "âœ… Updated 2662 books so far...\n",
      "âœ… Updated 2761 books so far...\n",
      "âœ… Updated 2861 books so far...\n",
      "âœ… Updated 2961 books so far...\n",
      "âœ… Updated 3059 books so far...\n",
      "âœ… Updated 3158 books so far...\n",
      "âœ… Updated 3258 books so far...\n",
      "âœ… Updated 3358 books so far...\n",
      "âœ… Updated 3455 books so far...\n",
      "âœ… Updated 3551 books so far...\n",
      "âœ… Updated 3651 books so far...\n",
      "âœ… Updated 3749 books so far...\n",
      "âœ… Updated 3847 books so far...\n",
      "âœ… Updated 3945 books so far...\n",
      "âœ… Updated 4045 books so far...\n",
      "âœ… Updated 4144 books so far...\n",
      "âœ… Updated 4241 books so far...\n",
      "âœ… Updated 4340 books so far...\n",
      "âœ… Updated 4439 books so far...\n",
      "âœ… Updated 4539 books so far...\n",
      "âœ… Updated 4638 books so far...\n",
      "âœ… Updated 4738 books so far...\n",
      "âœ… Updated 4836 books so far...\n",
      "âœ… Updated 4936 books so far...\n",
      "âœ… Updated 5035 books so far...\n",
      "âœ… Updated 5134 books so far...\n",
      "âœ… Updated 5233 books so far...\n",
      "âœ… Updated 5331 books so far...\n",
      "âœ… Updated 5430 books so far...\n",
      "âœ… Updated 5529 books so far...\n",
      "âœ… Updated 5629 books so far...\n",
      "âœ… Updated 5729 books so far...\n",
      "âœ… Updated 5829 books so far...\n",
      "âœ… Updated 5927 books so far...\n",
      "âœ… Updated 6026 books so far...\n",
      "âœ… Updated 6125 books so far...\n",
      "âœ… Updated 6224 books so far...\n",
      "âœ… Updated 6324 books so far...\n",
      "âœ… Updated 6424 books so far...\n",
      "âœ… Updated 6522 books so far...\n",
      "âœ… Updated 6622 books so far...\n",
      "âœ… Updated 6720 books so far...\n",
      "âœ… Updated 6818 books so far...\n",
      "âœ… Updated 6918 books so far...\n",
      "âœ… Updated 7017 books so far...\n",
      "âœ… Updated 7116 books so far...\n",
      "âœ… Updated 7215 books so far...\n",
      "âœ… Updated 7314 books so far...\n",
      "âœ… Updated 7413 books so far...\n",
      "âœ… Updated 7512 books so far...\n",
      "âœ… Updated 7612 books so far...\n",
      "âœ… Updated 7710 books so far...\n",
      "âœ… Updated 7809 books so far...\n",
      "âœ… Updated 7909 books so far...\n",
      "âœ… Updated 8009 books so far...\n",
      "âœ… Updated 8109 books so far...\n",
      "âœ… Updated 8209 books so far...\n",
      "âœ… Updated 8309 books so far...\n",
      "âœ… Updated 8406 books so far...\n",
      "âœ… Updated 8506 books so far...\n",
      "âœ… Updated 8603 books so far...\n",
      "âœ… Updated 8701 books so far...\n",
      "âœ… Updated 8800 books so far...\n",
      "âœ… Updated 8899 books so far...\n",
      "âœ… Updated 8999 books so far...\n",
      "âœ… Updated 9098 books so far...\n",
      "âœ… Updated 9196 books so far...\n",
      "âœ… Updated 9296 books so far...\n",
      "âœ… Updated 9396 books so far...\n",
      "âœ… Updated 9495 books so far...\n",
      "âœ… Updated 9593 books so far...\n",
      "âœ… Updated 9691 books so far...\n",
      "âœ… Updated 9790 books so far...\n",
      "âœ… Updated 9890 books so far...\n",
      "âœ… Updated 9987 books so far...\n",
      "âœ… Updated 10087 books so far...\n",
      "âœ… Updated 10185 books so far...\n",
      "âœ… Updated 10283 books so far...\n",
      "âœ… Updated 10382 books so far...\n",
      "âœ… Updated 10481 books so far...\n",
      "âœ… Updated 10580 books so far...\n",
      "âœ… Updated 10676 books so far...\n",
      "âœ… Updated 10775 books so far...\n",
      "âœ… Updated 10871 books so far...\n",
      "âœ… Updated 10967 books so far...\n",
      "âœ… Updated 11065 books so far...\n",
      "âœ… Updated 11162 books so far...\n",
      "âœ… Updated 11261 books so far...\n",
      "âœ… Updated 11361 books so far...\n",
      "âœ… Updated 11461 books so far...\n",
      "âœ… Updated 11561 books so far...\n",
      "âœ… Updated 11660 books so far...\n",
      "âœ… Updated 11759 books so far...\n",
      "âœ… Updated 11856 books so far...\n",
      "âœ… Updated 11955 books so far...\n",
      "âœ… Updated 12055 books so far...\n",
      "âœ… Updated 12155 books so far...\n",
      "âœ… Updated 12253 books so far...\n",
      "âœ… Updated 12351 books so far...\n",
      "âœ… Updated 12450 books so far...\n",
      "âœ… Updated 12547 books so far...\n",
      "âœ… Updated 12647 books so far...\n",
      "âœ… Updated 12746 books so far...\n",
      "âœ… Updated 12843 books so far...\n",
      "âœ… Updated 12943 books so far...\n",
      "âœ… Updated 13043 books so far...\n",
      "âœ… Updated 13141 books so far...\n",
      "âœ… Updated 13240 books so far...\n",
      "âœ… Updated 13337 books so far...\n",
      "âœ… Updated 13436 books so far...\n",
      "âœ… Updated 13535 books so far...\n",
      "âœ… Updated 13635 books so far...\n",
      "âœ… Updated 13735 books so far...\n",
      "âœ… Updated 13834 books so far...\n",
      "âœ… Updated 13934 books so far...\n",
      "âœ… Updated 14033 books so far...\n",
      "âœ… Updated 14132 books so far...\n",
      "âœ… Updated 14231 books so far...\n",
      "âœ… Updated 14328 books so far...\n",
      "âœ… Updated 14423 books so far...\n",
      "âœ… Updated 14521 books so far...\n",
      "âœ… Updated 14618 books so far...\n",
      "âœ… Updated 14716 books so far...\n",
      "âœ… Updated 14816 books so far...\n",
      "âœ… Updated 14915 books so far...\n",
      "âœ… Updated 15014 books so far...\n",
      "âœ… Updated 15111 books so far...\n",
      "âœ… Updated 15209 books so far...\n",
      "âœ… Updated 15309 books so far...\n",
      "âœ… Updated 15408 books so far...\n",
      "âœ… Updated 15507 books so far...\n",
      "âœ… Updated 15607 books so far...\n",
      "âœ… Updated 15705 books so far...\n",
      "âœ… Updated 15805 books so far...\n",
      "âœ… Updated 15905 books so far...\n",
      "âœ… Updated 16004 books so far...\n",
      "âœ… Updated 16104 books so far...\n",
      "âœ… Updated 16204 books so far...\n",
      "âœ… Updated 16301 books so far...\n",
      "âœ… Updated 16401 books so far...\n",
      "âœ… Updated 16500 books so far...\n",
      "âœ… Updated 16600 books so far...\n",
      "âœ… Updated 16698 books so far...\n",
      "âœ… Updated 16796 books so far...\n",
      "âœ… Updated 16894 books so far...\n",
      "âœ… Updated 16994 books so far...\n",
      "âœ… Updated 17091 books so far...\n",
      "âœ… Updated 17190 books so far...\n",
      "âœ… Updated 17290 books so far...\n",
      "âœ… Updated 17389 books so far...\n",
      "âœ… Updated 17488 books so far...\n",
      "âœ… Updated 17587 books so far...\n",
      "âœ… Updated 17687 books so far...\n",
      "âœ… Updated 17786 books so far...\n",
      "âœ… Updated 17886 books so far...\n",
      "âœ… Updated 17985 books so far...\n",
      "âœ… Updated 18084 books so far...\n",
      "âœ… Updated 18181 books so far...\n",
      "âœ… Updated 18280 books so far...\n",
      "âœ… Updated 18380 books so far...\n",
      "âœ… Updated 18478 books so far...\n",
      "âœ… Updated 18576 books so far...\n",
      "âœ… Updated 18675 books so far...\n",
      "âœ… Updated 18774 books so far...\n",
      "âœ… Updated 18874 books so far...\n",
      "âœ… Updated 18973 books so far...\n",
      "âœ… Updated 19070 books so far...\n",
      "âœ… Updated 19169 books so far...\n",
      "âœ… Updated 19268 books so far...\n",
      "âœ… Updated 19368 books so far...\n",
      "âœ… Updated 19467 books so far...\n",
      "âœ… Updated 19566 books so far...\n",
      "âœ… Updated 19664 books so far...\n",
      "âœ… Updated 19764 books so far...\n",
      "ðŸ Finished. Total books updated: 19764\n"
     ]
    }
   ],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials, firestore\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from google.api_core.exceptions import ResourceExhausted\n",
    "import time\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# âœ… Custom SelfAttention Layer (used in training)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\n",
    "            name=\"att_weight\",\n",
    "            shape=(input_shape[-1], 1),\n",
    "            initializer=\"normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = tf.nn.tanh(tf.tensordot(x, self.W, axes=1))\n",
    "        alpha = tf.nn.softmax(e, axis=1)\n",
    "        return tf.reduce_sum(x * alpha, axis=1)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# âœ… Load Model & Tokenizer\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model = tf.keras.models.load_model(\n",
    "    \"emotion_model_1.h5\",\n",
    "    custom_objects={\n",
    "        \"SelfAttention\": SelfAttention,\n",
    "        \"mse\": MeanSquaredError()\n",
    "    }\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Extract model input names once\n",
    "model_input_names = [tensor.name.split(\":\")[0] for tensor in model.inputs]\n",
    "print(\"ðŸ§  Model input names:\", model_input_names)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# âœ… Initialize Firebase Admin\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cred = credentials.Certificate(\"books-recommend-61b40-firebase-adminsdk-fbsvc-a1abc2d3ab.json\")\n",
    "if not firebase_admin._apps:\n",
    "    firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()\n",
    "books_ref = db.collection(\"books\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# âœ… Batch Update with Pagination\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BATCH_SIZE = 250\n",
    "FETCH_LIMIT = 100\n",
    "MAX_TOKENS = 128\n",
    "total_updated = 0\n",
    "last_doc = None\n",
    "\n",
    "print(\"ðŸš€ Starting emotion vector update...\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        query = books_ref.limit(FETCH_LIMIT)\n",
    "        if last_doc:\n",
    "            query = query.start_after(last_doc)\n",
    "\n",
    "        # Try fetching docs with quota retry\n",
    "        for retry in range(3):\n",
    "            try:\n",
    "                docs = list(query.stream())\n",
    "                break\n",
    "            except ResourceExhausted:\n",
    "                print(f\"âš ï¸ Read quota exceeded. Waiting 60s... (retry {retry+1}/3)\")\n",
    "                time.sleep(60)\n",
    "        else:\n",
    "            print(\"âŒ Giving up on reading docs due to repeated quota errors.\")\n",
    "            break\n",
    "\n",
    "        if not docs:\n",
    "            break\n",
    "\n",
    "        batch = db.batch()\n",
    "\n",
    "        for doc_snapshot in docs:\n",
    "            doc = doc_snapshot.to_dict()\n",
    "            doc_ref = books_ref.document(doc_snapshot.id)\n",
    "\n",
    "            # Skip if already processed or invalid description\n",
    "            if \"emotion_vector\" in doc or not isinstance(doc.get(\"description\"), str):\n",
    "                continue\n",
    "\n",
    "            input_text = doc[\"description\"]\n",
    "            # Tokenize using Keras tokenizer\n",
    "            # Tokenize using Hugging Face tokenizer\n",
    "            tokens = tokenizer(\n",
    "                input_text,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=MAX_TOKENS,\n",
    "                return_tensors=\"np\"\n",
    "            )\n",
    "\n",
    "            # Use only input_ids as required by your model\n",
    "            input_ids = tokens[\"input_ids\"]\n",
    "\n",
    "            # Clip large token values (to prevent embedding lookup error)\n",
    "            input_ids = np.clip(input_ids, 0, 9999)\n",
    "\n",
    "            try:\n",
    "                prediction = model.predict(input_ids, verbose=0)[0].tolist()\n",
    "                batch.update(doc_ref, {\"emotion_vector\": prediction})\n",
    "                total_updated += 1\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Prediction error for doc {doc_snapshot.id}: {e}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Prediction error for doc {doc_snapshot.id}: {e}\")\n",
    "                continue\n",
    "\n",
    "        batch.commit()\n",
    "        last_doc = docs[-1]\n",
    "        print(f\"âœ… Updated {total_updated} books so far...\")\n",
    "\n",
    "    except ResourceExhausted:\n",
    "        print(\"âš ï¸ Write quota exceeded. Waiting 60 seconds before retrying...\")\n",
    "        time.sleep(60)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Unexpected error: {e}\")\n",
    "        time.sleep(5)\n",
    "\n",
    "print(f\"ðŸ Finished. Total books updated: {total_updated}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb9983f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Exported 19769 books to books.json\n"
     ]
    }
   ],
   "source": [
    "# import firebase_admin\n",
    "# from firebase_admin import credentials, firestore\n",
    "# import json\n",
    "\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# # âœ… Initialize Firebase\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# cred = credentials.Certificate(\"books-recommend-61b40-firebase-adminsdk-fbsvc-a1abc2d3ab.json\")\n",
    "# if not firebase_admin._apps:\n",
    "#     firebase_admin.initialize_app(cred)\n",
    "\n",
    "# db = firestore.client()\n",
    "\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# # âœ… Export books with emotion_vector\n",
    "# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# books_ref = db.collection(\"books\").stream()\n",
    "# books = []\n",
    "\n",
    "# for doc in books_ref:\n",
    "#     data = doc.to_dict()\n",
    "#     if \"emotion_vector\" in data:\n",
    "#         books.append(data)\n",
    "\n",
    "# # Save to books.json\n",
    "# with open(\"books.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(books, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# print(f\"âœ… Exported {len(books)} books to books.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
