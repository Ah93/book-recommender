{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dc3bc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"books_with_emotions.json\"\n",
    "output_file = \"books_with_emotions_fixed.json\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"[\\n\")\n",
    "    for i, line in enumerate(lines):\n",
    "        if i < len(lines) - 1:\n",
    "            f.write(line + \",\\n\")\n",
    "        else:\n",
    "            f.write(line + \"\\n\")\n",
    "    f.write(\"]\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ec6154a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Model input names: ['input_layer']\n",
      "🚀 Starting emotion vector update...\n",
      "✅ Updated 99 books so far...\n",
      "✅ Updated 194 books so far...\n",
      "✅ Updated 294 books so far...\n",
      "✅ Updated 391 books so far...\n",
      "✅ Updated 490 books so far...\n",
      "✅ Updated 589 books so far...\n",
      "✅ Updated 688 books so far...\n",
      "✅ Updated 786 books so far...\n",
      "✅ Updated 884 books so far...\n",
      "✅ Updated 983 books so far...\n",
      "✅ Updated 1080 books so far...\n",
      "✅ Updated 1180 books so far...\n",
      "✅ Updated 1278 books so far...\n",
      "✅ Updated 1377 books so far...\n",
      "✅ Updated 1477 books so far...\n",
      "✅ Updated 1574 books so far...\n",
      "✅ Updated 1673 books so far...\n",
      "✅ Updated 1772 books so far...\n",
      "✅ Updated 1870 books so far...\n",
      "✅ Updated 1967 books so far...\n",
      "✅ Updated 2067 books so far...\n",
      "✅ Updated 2167 books so far...\n",
      "✅ Updated 2267 books so far...\n",
      "✅ Updated 2366 books so far...\n",
      "✅ Updated 2465 books so far...\n",
      "✅ Updated 2563 books so far...\n",
      "✅ Updated 2662 books so far...\n",
      "✅ Updated 2761 books so far...\n",
      "✅ Updated 2861 books so far...\n",
      "✅ Updated 2961 books so far...\n",
      "✅ Updated 3059 books so far...\n",
      "✅ Updated 3158 books so far...\n",
      "✅ Updated 3258 books so far...\n",
      "✅ Updated 3358 books so far...\n",
      "✅ Updated 3455 books so far...\n",
      "✅ Updated 3551 books so far...\n",
      "✅ Updated 3651 books so far...\n",
      "✅ Updated 3749 books so far...\n",
      "✅ Updated 3847 books so far...\n",
      "✅ Updated 3945 books so far...\n",
      "✅ Updated 4045 books so far...\n",
      "✅ Updated 4144 books so far...\n",
      "✅ Updated 4241 books so far...\n",
      "✅ Updated 4340 books so far...\n",
      "✅ Updated 4439 books so far...\n",
      "✅ Updated 4539 books so far...\n",
      "✅ Updated 4638 books so far...\n",
      "✅ Updated 4738 books so far...\n",
      "✅ Updated 4836 books so far...\n",
      "✅ Updated 4936 books so far...\n",
      "✅ Updated 5035 books so far...\n",
      "✅ Updated 5134 books so far...\n",
      "✅ Updated 5233 books so far...\n",
      "✅ Updated 5331 books so far...\n",
      "✅ Updated 5430 books so far...\n",
      "✅ Updated 5529 books so far...\n",
      "✅ Updated 5629 books so far...\n",
      "✅ Updated 5729 books so far...\n",
      "✅ Updated 5829 books so far...\n",
      "✅ Updated 5927 books so far...\n",
      "✅ Updated 6026 books so far...\n",
      "✅ Updated 6125 books so far...\n",
      "✅ Updated 6224 books so far...\n",
      "✅ Updated 6324 books so far...\n",
      "✅ Updated 6424 books so far...\n",
      "✅ Updated 6522 books so far...\n",
      "✅ Updated 6622 books so far...\n",
      "✅ Updated 6720 books so far...\n",
      "✅ Updated 6818 books so far...\n",
      "✅ Updated 6918 books so far...\n",
      "✅ Updated 7017 books so far...\n",
      "✅ Updated 7116 books so far...\n",
      "✅ Updated 7215 books so far...\n",
      "✅ Updated 7314 books so far...\n",
      "✅ Updated 7413 books so far...\n",
      "✅ Updated 7512 books so far...\n",
      "✅ Updated 7612 books so far...\n",
      "✅ Updated 7710 books so far...\n",
      "✅ Updated 7809 books so far...\n",
      "✅ Updated 7909 books so far...\n",
      "✅ Updated 8009 books so far...\n",
      "✅ Updated 8109 books so far...\n",
      "✅ Updated 8209 books so far...\n",
      "✅ Updated 8309 books so far...\n",
      "✅ Updated 8406 books so far...\n",
      "✅ Updated 8506 books so far...\n",
      "✅ Updated 8603 books so far...\n",
      "✅ Updated 8701 books so far...\n",
      "✅ Updated 8800 books so far...\n",
      "✅ Updated 8899 books so far...\n",
      "✅ Updated 8999 books so far...\n",
      "✅ Updated 9098 books so far...\n",
      "✅ Updated 9196 books so far...\n",
      "✅ Updated 9296 books so far...\n",
      "✅ Updated 9396 books so far...\n",
      "✅ Updated 9495 books so far...\n",
      "✅ Updated 9593 books so far...\n",
      "✅ Updated 9691 books so far...\n",
      "✅ Updated 9790 books so far...\n",
      "✅ Updated 9890 books so far...\n",
      "✅ Updated 9987 books so far...\n",
      "✅ Updated 10087 books so far...\n",
      "✅ Updated 10185 books so far...\n",
      "✅ Updated 10283 books so far...\n",
      "✅ Updated 10382 books so far...\n",
      "✅ Updated 10481 books so far...\n",
      "✅ Updated 10580 books so far...\n",
      "✅ Updated 10676 books so far...\n",
      "✅ Updated 10775 books so far...\n",
      "✅ Updated 10871 books so far...\n",
      "✅ Updated 10967 books so far...\n",
      "✅ Updated 11065 books so far...\n",
      "✅ Updated 11162 books so far...\n",
      "✅ Updated 11261 books so far...\n",
      "✅ Updated 11361 books so far...\n",
      "✅ Updated 11461 books so far...\n",
      "✅ Updated 11561 books so far...\n",
      "✅ Updated 11660 books so far...\n",
      "✅ Updated 11759 books so far...\n",
      "✅ Updated 11856 books so far...\n",
      "✅ Updated 11955 books so far...\n",
      "✅ Updated 12055 books so far...\n",
      "✅ Updated 12155 books so far...\n",
      "✅ Updated 12253 books so far...\n",
      "✅ Updated 12351 books so far...\n",
      "✅ Updated 12450 books so far...\n",
      "✅ Updated 12547 books so far...\n",
      "✅ Updated 12647 books so far...\n",
      "✅ Updated 12746 books so far...\n",
      "✅ Updated 12843 books so far...\n",
      "✅ Updated 12943 books so far...\n",
      "✅ Updated 13043 books so far...\n",
      "✅ Updated 13141 books so far...\n",
      "✅ Updated 13240 books so far...\n",
      "✅ Updated 13337 books so far...\n",
      "✅ Updated 13436 books so far...\n",
      "✅ Updated 13535 books so far...\n",
      "✅ Updated 13635 books so far...\n",
      "✅ Updated 13735 books so far...\n",
      "✅ Updated 13834 books so far...\n",
      "✅ Updated 13934 books so far...\n",
      "✅ Updated 14033 books so far...\n",
      "✅ Updated 14132 books so far...\n",
      "✅ Updated 14231 books so far...\n",
      "✅ Updated 14328 books so far...\n",
      "✅ Updated 14423 books so far...\n",
      "✅ Updated 14521 books so far...\n",
      "✅ Updated 14618 books so far...\n",
      "✅ Updated 14716 books so far...\n",
      "✅ Updated 14816 books so far...\n",
      "✅ Updated 14915 books so far...\n",
      "✅ Updated 15014 books so far...\n",
      "✅ Updated 15111 books so far...\n",
      "✅ Updated 15209 books so far...\n",
      "✅ Updated 15309 books so far...\n",
      "✅ Updated 15408 books so far...\n",
      "✅ Updated 15507 books so far...\n",
      "✅ Updated 15607 books so far...\n",
      "✅ Updated 15705 books so far...\n",
      "✅ Updated 15805 books so far...\n",
      "✅ Updated 15905 books so far...\n",
      "✅ Updated 16004 books so far...\n",
      "✅ Updated 16104 books so far...\n",
      "✅ Updated 16204 books so far...\n",
      "✅ Updated 16301 books so far...\n",
      "✅ Updated 16401 books so far...\n",
      "✅ Updated 16500 books so far...\n",
      "✅ Updated 16600 books so far...\n",
      "✅ Updated 16698 books so far...\n",
      "✅ Updated 16796 books so far...\n",
      "✅ Updated 16894 books so far...\n",
      "✅ Updated 16994 books so far...\n",
      "✅ Updated 17091 books so far...\n",
      "✅ Updated 17190 books so far...\n",
      "✅ Updated 17290 books so far...\n",
      "✅ Updated 17389 books so far...\n",
      "✅ Updated 17488 books so far...\n",
      "✅ Updated 17587 books so far...\n",
      "✅ Updated 17687 books so far...\n",
      "✅ Updated 17786 books so far...\n",
      "✅ Updated 17886 books so far...\n",
      "✅ Updated 17985 books so far...\n",
      "✅ Updated 18084 books so far...\n",
      "✅ Updated 18181 books so far...\n",
      "✅ Updated 18280 books so far...\n",
      "✅ Updated 18380 books so far...\n",
      "✅ Updated 18478 books so far...\n",
      "✅ Updated 18576 books so far...\n",
      "✅ Updated 18675 books so far...\n",
      "✅ Updated 18774 books so far...\n",
      "✅ Updated 18874 books so far...\n",
      "✅ Updated 18973 books so far...\n",
      "✅ Updated 19070 books so far...\n",
      "✅ Updated 19169 books so far...\n",
      "✅ Updated 19268 books so far...\n",
      "✅ Updated 19368 books so far...\n",
      "✅ Updated 19467 books so far...\n",
      "✅ Updated 19566 books so far...\n",
      "✅ Updated 19664 books so far...\n",
      "✅ Updated 19764 books so far...\n",
      "🏁 Finished. Total books updated: 19764\n"
     ]
    }
   ],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials, firestore\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from google.api_core.exceptions import ResourceExhausted\n",
    "import time\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# ✅ Custom SelfAttention Layer (used in training)\n",
    "# ───────────────────────────────────────────────\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\n",
    "            name=\"att_weight\",\n",
    "            shape=(input_shape[-1], 1),\n",
    "            initializer=\"normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = tf.nn.tanh(tf.tensordot(x, self.W, axes=1))\n",
    "        alpha = tf.nn.softmax(e, axis=1)\n",
    "        return tf.reduce_sum(x * alpha, axis=1)\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# ✅ Load Model & Tokenizer\n",
    "# ───────────────────────────────────────────────\n",
    "model = tf.keras.models.load_model(\n",
    "    \"emotion_model_1.h5\",\n",
    "    custom_objects={\n",
    "        \"SelfAttention\": SelfAttention,\n",
    "        \"mse\": MeanSquaredError()\n",
    "    }\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Extract model input names once\n",
    "model_input_names = [tensor.name.split(\":\")[0] for tensor in model.inputs]\n",
    "print(\"🧠 Model input names:\", model_input_names)\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# ✅ Initialize Firebase Admin\n",
    "# ───────────────────────────────────────────────\n",
    "cred = credentials.Certificate(\"books-recommend-61b40-firebase-adminsdk-fbsvc-a1abc2d3ab.json\")\n",
    "if not firebase_admin._apps:\n",
    "    firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()\n",
    "books_ref = db.collection(\"books\")\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# ✅ Batch Update with Pagination\n",
    "# ───────────────────────────────────────────────\n",
    "BATCH_SIZE = 250\n",
    "FETCH_LIMIT = 100\n",
    "MAX_TOKENS = 128\n",
    "total_updated = 0\n",
    "last_doc = None\n",
    "\n",
    "print(\"🚀 Starting emotion vector update...\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        query = books_ref.limit(FETCH_LIMIT)\n",
    "        if last_doc:\n",
    "            query = query.start_after(last_doc)\n",
    "\n",
    "        # Try fetching docs with quota retry\n",
    "        for retry in range(3):\n",
    "            try:\n",
    "                docs = list(query.stream())\n",
    "                break\n",
    "            except ResourceExhausted:\n",
    "                print(f\"⚠️ Read quota exceeded. Waiting 60s... (retry {retry+1}/3)\")\n",
    "                time.sleep(60)\n",
    "        else:\n",
    "            print(\"❌ Giving up on reading docs due to repeated quota errors.\")\n",
    "            break\n",
    "\n",
    "        if not docs:\n",
    "            break\n",
    "\n",
    "        batch = db.batch()\n",
    "\n",
    "        for doc_snapshot in docs:\n",
    "            doc = doc_snapshot.to_dict()\n",
    "            doc_ref = books_ref.document(doc_snapshot.id)\n",
    "\n",
    "            # Skip if already processed or invalid description\n",
    "            if \"emotion_vector\" in doc or not isinstance(doc.get(\"description\"), str):\n",
    "                continue\n",
    "\n",
    "            input_text = doc[\"description\"]\n",
    "            # Tokenize using Keras tokenizer\n",
    "            # Tokenize using Hugging Face tokenizer\n",
    "            tokens = tokenizer(\n",
    "                input_text,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=MAX_TOKENS,\n",
    "                return_tensors=\"np\"\n",
    "            )\n",
    "\n",
    "            # Use only input_ids as required by your model\n",
    "            input_ids = tokens[\"input_ids\"]\n",
    "\n",
    "            # Clip large token values (to prevent embedding lookup error)\n",
    "            input_ids = np.clip(input_ids, 0, 9999)\n",
    "\n",
    "            try:\n",
    "                prediction = model.predict(input_ids, verbose=0)[0].tolist()\n",
    "                batch.update(doc_ref, {\"emotion_vector\": prediction})\n",
    "                total_updated += 1\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Prediction error for doc {doc_snapshot.id}: {e}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Prediction error for doc {doc_snapshot.id}: {e}\")\n",
    "                continue\n",
    "\n",
    "        batch.commit()\n",
    "        last_doc = docs[-1]\n",
    "        print(f\"✅ Updated {total_updated} books so far...\")\n",
    "\n",
    "    except ResourceExhausted:\n",
    "        print(\"⚠️ Write quota exceeded. Waiting 60 seconds before retrying...\")\n",
    "        time.sleep(60)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Unexpected error: {e}\")\n",
    "        time.sleep(5)\n",
    "\n",
    "print(f\"🏁 Finished. Total books updated: {total_updated}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb9983f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported 19769 books to books.json\n"
     ]
    }
   ],
   "source": [
    "# import firebase_admin\n",
    "# from firebase_admin import credentials, firestore\n",
    "# import json\n",
    "\n",
    "# # ───────────────────────────────────────────────\n",
    "# # ✅ Initialize Firebase\n",
    "# # ───────────────────────────────────────────────\n",
    "# cred = credentials.Certificate(\"books-recommend-61b40-firebase-adminsdk-fbsvc-a1abc2d3ab.json\")\n",
    "# if not firebase_admin._apps:\n",
    "#     firebase_admin.initialize_app(cred)\n",
    "\n",
    "# db = firestore.client()\n",
    "\n",
    "# # ───────────────────────────────────────────────\n",
    "# # ✅ Export books with emotion_vector\n",
    "# # ───────────────────────────────────────────────\n",
    "# books_ref = db.collection(\"books\").stream()\n",
    "# books = []\n",
    "\n",
    "# for doc in books_ref:\n",
    "#     data = doc.to_dict()\n",
    "#     if \"emotion_vector\" in data:\n",
    "#         books.append(data)\n",
    "\n",
    "# # Save to books.json\n",
    "# with open(\"books.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(books, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# print(f\"✅ Exported {len(books)} books to books.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
